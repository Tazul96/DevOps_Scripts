This is My first deployment in contabo VPS with 1 master node and 2 worker nodes. It isn't manage k8s cluster. I setup from the sketch.

###Create k8s user
sudo adduser k8s
sudo usermod -aG sudo k8s
su - k8s


### Update and Install docker demon or engine
sudo apt update
sudo apt install docker.io -y
sudo systemctl enable docker
sudo systemctl status docker
sudo systemctl start docker


### Install Kubernatics and assoiciate settings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt update

sudo apt install kubeadm kubelet kubectl
sudo apt-mark hold kubeadm kubelet kubectl


## Disable the swap memory that willn't use SSD if memory is not shortage
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


#Add netfilter
sudo nano /etc/modules-load.d/containerd.conf
overlay
br_netfilter

sudo modprobe overlay
sudo modprobe br_netfilter

sudo nano /etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

sudo sysctl --system

###Set Host machine name
sudo hostnamectl set-hostname master-node #In master-node
sudo hostnamectl set-hostname worker-1 #In worker 1
sudo hostnamectl set-hostname worker02 #In worker 2


sudo nano /etc/hosts 
-- Add master note and server ip6tables
	127.0.0.1       localhost
	127.0.1.1       vmi2285330.contaboserver.net    vmi2285330
	IP-masternode   master-node
	IP-workernode  worker-1
	IP-workernode  worker-2

	# The following lines are desirable for IPv6 capable hosts
	::1     localhost ip6-localhost ip6-loopback ff02::1 ip6-allnodes ff02::2 ip6-allrouters


sudo reboot

##Set up environment for control groups to containerized processes in .
sudo nano /etc/default/kubelet

KUBELET_EXTRA_ARGS="--cgroup-driver=cgroupfs"

sudo systemctl daemon-reload && sudo systemctl restart kubelet

sudo nano /etc/docker/daemon.json
	{
      "exec-opts": ["native.cgroupdriver=systemd"],
      "log-driver": "json-file",
      "log-opts": {
      "max-size": "100m"
   },
       "storage-driver": "overlay2"
       }
	   
	   
sudo systemctl daemon-reload && sudo systemctl restart docker

sudo nano /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

sudo systemctl restart systemd-networkd

sudo mkdir -p /etc/systemd/system/kubelet.service.d 

sudo cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf ( copy the file and add a new section annd submit in next step.)

# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS




sudo nano /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
	Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"


sudo systemctl daemon-reload
sudo systemctl restart kubelet

sudo systemctl daemon-reload && sudo systemctl restart kubelet

sudo kubeadm init --control-plane-endpoint=master-node --upload-certs

or 

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 #if don't have network created yet or create network automatically after init

Find the token:
kubeadm token create --print-join-command

kubeadm join master-node:6443 --token gurardim \
        --discovery-token-ca-cert-hash 
        murgir&haserdim


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

or 

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml  #if want to setup calico network

kubectl delete -f https://docs.projectcalico.org/manifests/calico.yaml


Troubleshoot:

lsmod | grep iptable
modprobe iptable_filter
modprobe iptable_nat
modprobe iptable_mangle
modprobe ip6table_filter


kubectl taint nodes --all node-role.kubernetes.io/control-plane-


#Worker node
mkdir -p ~/.kube
scp user@master-node:/etc/kubernetes/admin.conf ~/.kube/config



kubectl create secret docker-registry dockerhub-secret --namespace=test \
  --docker-username=dibonah \
  --docker-password=bolbonah \
  --docker-email=ttazulislam96@gmail.com



kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

kubectl edit service kubernetes-dashboard -n kubernetes-dashboard

nodePort: 30001 under ports.
changed type: ClusterIP to type: NodePort.


kubectl get services --all-namespaces -o jsonpath='{range .items[*]}{.spec.ports[*].nodePort}{"\n"}{end}'
kubectl get services -n kubernetes-dashboard

vim kubernetes-dashboard-admin-user.yml
	apiVersion: v1
	kind: ServiceAccount
	metadata:
	  name: admin-user
	  namespace: kubernetes-dashboard
	---
	apiVersion: rbac.authorization.k8s.io/v1
	kind: ClusterRoleBinding
	metadata:
	  name: admin-user
	roleRef:
	  apiGroup: rbac.authorization.k8s.io
	  kind: ClusterRole
	  name: cluster-admin
	subjects:
	- kind: ServiceAccount
	  name: admin-user
	  namespace: kubernetes-dashboard


kubectl apply -f kubernetes-dashboard-admin-user.yml
kubectl get serviceaccounts -n kubernetes-dashboard
kubectl create token admin-user -n kubernetes-dashboard


https://Node-IP:30001.


 kubectl get service -n ingress-nginx ingress-nginx-controller


		
		
		
		
kubectl get pods -n metallb-system


See all images in the cluster
kubectl get pods --all-namespaces -o=jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n'

kubeadm reset -f
rm -rf /etc/kubernetes/pki/
ipvsadm --clear
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

sudo rm -rf /etc/cni/net.d/calico*
sudo rm -rf /var/lib/calico
sudo rm -rf /etc/cni/net.d


# Download Calico CNI binaries
sudo mkdir -p /opt/cni/bin
sudo curl -L https://github.com/projectcalico/cni-plugin/releases/download/v3.26.1/calico-amd64 -o /opt/cni/bin/calico
sudo curl -L https://github.com/projectcalico/cni-plugin/releases/download/v3.26.1/calico-ipam-amd64 -o /opt/cni/bin/calico-ipam
sudo chmod +x /opt/cni/bin/calico*

# Create CNI config
sudo mkdir -p /etc/cni/net.d
sudo curl -L https://docs.projectcalico.org/manifests/cni-default-network.yaml -o /etc/cni/net.d/10-calico.conflist